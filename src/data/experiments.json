[
    {
        "id": "wcst",
        "task": "[Executive Function] (Cognitive Flexibility) WCST",
        "result": "All models tested have sub-stantially lower accuracy than humans (70 - 80%). Unlike humans, models do not appear to do very well adapting to changing sorting rules: we find no correlation between model accuracy and number of turns exposed to a given rule.",
        "insights": [
            {
                "src": "/images/WCST.png",
                "alt": "",
                "text": "Accuracy for each turn in the dialogue for Qwen2-72B. Colors indicate underlying sorting rule for that turn. The models do not adapt well to the implicit rule changes, and the accuracy declines as the dialogue continues."
            }
        ]
    },
    {
        "id": "flanker",
        "task": "[Executive Function] (Attentional Control) Flanker Task",
        "result": "",
        "insights": [
            {
                "src": "/images/flanker.png",
                "alt": "",
                "text": "Model performance is near perfect for the congruent letter strings, but has accuracy of around 40-60% for the incongruent letter strings across all models tested, substantially below human baselines (93 - 96%)."
            },
            {
                "src": "/images/flanker_probs.png",
                "alt": "",
                "text": "When models answer correctly, model estimates of the correct answerâ€™s probability still tend to be lower in the incongruent condition."
            }
        ]
    },
    {
        "id": "digit",
        "task": "[Executive Function & Memory] (Working & Short-term Memory) Digit Span Tasks",
        "result": "In the forward digit span task, all LLMs tested have nearly perfect responses for all digit lengths (up to 50) while the mean for human is seven. In the backward digit span task, smaller models have decreased accuracy after 15-20 digit (mean backward digit span of human is 5).  Like humans, LLMs find the reversal operation makes this task more difficult.",
        "insights": [
            {
                "src": "/images/reverse-digit.png",
                "alt": "",
                "text": "Mean model accuracy, by digit sequence length, in the backward digit span task. Larger models do better at the backward digit task than their smaller counterparts."
            },
            {
                "src": "/images/reverse_perplexity.png",
                "alt": "",
                "text": "Perplexity of the correct sequences in the backward digit span task yields more nuance, e.g., note that Qwen2-7B accuracy is equal for lengths 15, 20, and 30, the perplexities indicate that its understanding of 30-length digits is worse."
            }
        ]
    },
    {
        "id": "drm",
        "task": "[Memory] (False/Gist Memory) DRM Task",
        "result": "Although smaller LLMs have decreased accuracy in their responses to the critical words, larger LLMs have nearly perfect performance. The responses indicate that larger LLMs are not susceptible to human-like false reports triggered by the semantic interference of the critical word (humans recognize unseen critical words 70% of the time - 30% accuracy)",
        "insights": [
            {
                "src": "/images/drm.png",
                "alt": "",
                "text": "DRM accuracies across different conditions. Note the difference in accuracy between the unseen control and unseen critical words, indicating susceptibility to false memory."
            }
        ]
    }
]